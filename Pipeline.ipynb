{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import warnings\n",
    "import time\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(row_csv, trainData_csv, testData_csv):\n",
    "    \"\"\"\n",
    "        row_csv:           a csv file containing target column index.\n",
    "        trainData_csv:     training dataset, csv file.\n",
    "        testData_csv:      testing dataset, csv file.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # display\n",
    "    print(\"Part I: Preprocessing\")\n",
    "    print(\"Start preprocessing...\")\n",
    "    \n",
    "    # create dataframe\n",
    "    try:\n",
    "        train_df = pd.read_csv(trainData_csv, encoding = \"latin1\")\n",
    "    except:\n",
    "        print(\"No data detected! Please put this .py at the root directory, such as the netid directory.\")\n",
    "        os._exit(0)\n",
    "    else:\n",
    "        print(\"Train Data Loading Success.\")\n",
    "    \n",
    "    try:\n",
    "        test_df = pd.read_csv(testData_csv)\n",
    "    except:\n",
    "        testData_csv = None\n",
    "        print(\"Detected no test data!\")\n",
    "    else:\n",
    "        print(\"Test Data Loading Success.\")\n",
    "        \n",
    "    try:\n",
    "        row = pd.read_csv(row_csv)\n",
    "    except:\n",
    "        print(\"Missing row.csv!\")\n",
    "        os._exit(0)\n",
    "    else:\n",
    "        print(\"Row.csv Loading Success.\")\n",
    "        \n",
    "    target_index = int(row[\"targetIndex\"])\n",
    "    \n",
    "    # get features\n",
    "    features = list(train_df.columns)\n",
    "    \n",
    "    # target column\n",
    "    target_col = [features[target_index]]\n",
    "    target = train_df[target_col[0]]\n",
    "    features.remove(target_col[0])\n",
    "    train_df.drop(target_col, axis = 1, inplace = True)\n",
    "    \n",
    "    # combine train and test\n",
    "    combined_train_test = [train_df, test_df] if testData_csv else [train_df]\n",
    "    \n",
    "    # replace spaces with null value\n",
    "    # if too much nan value, we can drop the feature\n",
    "    for dataset in combined_train_test:\n",
    "        for feature in dataset.columns:\n",
    "            dataset[feature] = dataset[feature].replace(\" \", np.nan)\n",
    "            dataset[feature] = dataset[feature].replace(\"?\", np.nan)\n",
    "            if dataset[feature].isna().sum()/dataset.shape[0] >= .5:\n",
    "                dataset.drop([feature], axis = 1, inplace = True)\n",
    "                \n",
    "    features = list(train_df.columns)\n",
    "    \n",
    "    # numerical columns\n",
    "    numerical_cols = list(train_df.select_dtypes(exclude = \"object\").columns)\n",
    "    numerical_cols = [x for x in numerical_cols if train_df[x].nunique() >= 6]\n",
    "    \n",
    "    # categorical columns\n",
    "    categorical_cols = [x for x in features if not x in numerical_cols]\n",
    "    \n",
    "    # binary columns\n",
    "    bin_cols = [x for x in categorical_cols if train_df[x].nunique()/train_df.shape[0] <= .1]\n",
    "    \n",
    "    # multivalues columns\n",
    "    mul_cols = [x for x in categorical_cols if not x in bin_cols]\n",
    "    \n",
    "    # fill in missing values\n",
    "    for dataset in combined_train_test:\n",
    "        for feature in numerical_cols:\n",
    "            dataset[feature].fillna(dataset[feature].median(), inplace = True)\n",
    "        for feature in categorical_cols:\n",
    "            dataset[feature].fillna(dataset[feature].mode()[0], inplace = True)\n",
    "    \n",
    "    # normalization and label encode\n",
    "    from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "    scaler, encoder = StandardScaler(), LabelEncoder()\n",
    "    for dataset in combined_train_test:\n",
    "        for feature in numerical_cols:\n",
    "            dataset[feature] = scaler.fit_transform(dataset[feature].values.reshape(-1, 1))\n",
    "        for feature in bin_cols:\n",
    "            dataset[feature] = encoder.fit_transform(dataset[feature])\n",
    "    target = encoder.fit_transform(target)\n",
    "\n",
    "    # pack dataset for next step\n",
    "    if len(combined_train_test) == 1:\n",
    "        data = [train_df, target]\n",
    "    else:\n",
    "        data = [train_df, target, test_df]\n",
    "    cols = [numerical_cols, bin_cols, mul_cols, target_col]\n",
    "    \n",
    "    # display\n",
    "    print(\"Preprocessing Done!\\n\")\n",
    "    print(\"We have the target feature \", target_col, \".\\n\")\n",
    "    print(\"The categorical features are\", categorical_cols, \"and numerical features are\", numerical_cols, \".\\n\")\n",
    "    print(\"Specifically in categorical features, we have features\", \n",
    "          bin_cols, \", which have several unique values\",\n",
    "          \" and\", mul_cols, \", which have plenty of unique values.\\n\")\n",
    "    \n",
    "    return data, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(data, cols):\n",
    "    \"\"\"\n",
    "        data:    a pack containing training set or/and testing set.\n",
    "        cols:    a list containing types of feature labels.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # display\n",
    "    print(\"Part II: Feature Extraction\")\n",
    "    print(\"Start feature extraction...\")\n",
    "    \n",
    "    # unpack data\n",
    "    train_df, target = data[0], data[1]\n",
    "    if len(data) == 3:\n",
    "        test_df = data[2]\n",
    "        combined_train_test = [train_df, test_df]\n",
    "    else:\n",
    "        combined_train_test = [train_df]\n",
    "    numerical_cols, target_col = cols[0], cols[3]\n",
    "    bin_cols, mul_cols = cols[1], cols[2]\n",
    "    \n",
    "    # if name feature exists, extract information from name\n",
    "    def get_title(name):\n",
    "        title_search = re.search(\"([A-Za-z]+)\\.\", name)\n",
    "        if title_search:\n",
    "            return title_search.group(1)\n",
    "        return \"\"\n",
    "    \n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    encoder = LabelEncoder()\n",
    "    NameFeature = []\n",
    "    for feature in mul_cols:\n",
    "        if \"name\" == feature.lower():\n",
    "            NameFeature.append(feature)\n",
    "            mul_cols.remove(feature)\n",
    "    \n",
    "    NewFeature = []\n",
    "    for feature in NameFeature:\n",
    "        name = feature + \"_Info\"\n",
    "        NewFeature.append(name)\n",
    "        for dataset in combined_train_test:\n",
    "            dataset[name] = dataset[feature].apply(get_title)\n",
    "            dataset[name] = dataset[name].replace([\"Sir\", \"Lady\", \"Don\", \"Rev\", \"Col\", \"Capt\", \"Countess\", \n",
    "                                                   \"Jonkheer\", \"Major\", \"Dr\", \"Dona\"], \"Others\")\n",
    "            dataset[name] = dataset[name].replace([\"Ms\"], \"Mrs\")\n",
    "            dataset[name] = dataset[name].replace([\"Mme\", \"Mlle\"], \"Miss\")\n",
    "            dataset[name] = encoder.fit_transform(dataset[name])\n",
    "            dataset.drop([feature], axis = 1, inplace = True)\n",
    "        bin_cols.append(name)\n",
    "    \n",
    "    # pack data for next step\n",
    "    if len(combined_train_test) == 1:\n",
    "        data = [train_df, target]\n",
    "    else:\n",
    "        data = [train_df, target, test_df]\n",
    "    cols = [numerical_cols, bin_cols, mul_cols, target_col]\n",
    "    \n",
    "    # display\n",
    "    print(\"Feature Extraction Done!\\n\")\n",
    "    if not NewFeature:\n",
    "        print(\"We don't get new features.\")\n",
    "    else:\n",
    "        print(\"We get new features\", NewFeature, \"and replace the original features respectively.\\n\")\n",
    "    \n",
    "    return data, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(data, cols):\n",
    "    \"\"\"\n",
    "        data:    a pack containing training set or/and testing set.\n",
    "        cols:    a list containing types of feature labels.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # display\n",
    "    print(\"Part III: Feature Selection\")\n",
    "    print(\"Start feature selection...\")\n",
    "    \n",
    "    # unpack data\n",
    "    train_df, target = data[0], data[1]\n",
    "    if len(data) == 3:\n",
    "        test_df = data[2]\n",
    "        combined_train_test = [train_df, test_df]\n",
    "    else:\n",
    "        combined_train_test = [train_df]\n",
    "    numerical_cols, target_col = cols[0], cols[3]\n",
    "    bin_cols, mul_cols = cols[1], cols[2]\n",
    "    \n",
    "    # drop id columns\n",
    "    IdFeature = []\n",
    "    for i in [numerical_cols, bin_cols, mul_cols]:\n",
    "        for feature in i:\n",
    "            if \"id\" in feature.lower():\n",
    "                IdFeature.append(feature)\n",
    "                i.remove(feature)\n",
    "    \n",
    "    for feature in IdFeature:\n",
    "        for dataset in combined_train_test:\n",
    "            dataset.drop([feature], axis = 1, inplace = True)\n",
    "    \n",
    "    # drop columns with same value\n",
    "    SameValueCol = []\n",
    "    for i in [numerical_cols, bin_cols, mul_cols]:\n",
    "        for feature in i:\n",
    "            for dataset in combined_train_test:\n",
    "                if (dataset[feature].nunique()) == 1:\n",
    "                    SameValueCol.append(feature)\n",
    "                    i.remove(feature)\n",
    "                    dataset.drop([feature], axis = 1, inplace = True)\n",
    "    \n",
    "   \n",
    "    # drop all multivalue columns\n",
    "    for feature in mul_cols:\n",
    "        for dataset in combined_train_test:\n",
    "            dataset.drop([feature], axis = 1, inplace = True)\n",
    "    mul_cols = []\n",
    "    \n",
    "    \n",
    "    # drop similar feature\n",
    "    features = [i for i in numerical_cols] + [j for j in bin_cols] + target_col\n",
    "    df = pd.concat([data[0], data[2]]) if len(data) == 3 else data[0]\n",
    "    \n",
    "    for feature in features:\n",
    "        elements = feature.lower().split(\" \")\n",
    "        if \"abbreviation\" in elements:\n",
    "            df.drop([feature], axis = 1, inplace = True)\n",
    "            features.remove(feature)\n",
    "        if \"code\" in elements:\n",
    "            tmp = feature.split(\" \")[0]\n",
    "            if tmp in features:\n",
    "                df.drop([tmp], axis = 1, inplace = True)\n",
    "                features.remove(tmp)\n",
    "    \n",
    "    features = list(df.columns)\n",
    "\n",
    "    # if too many features, do SelectKBest\n",
    "    from sklearn.feature_selection import SelectKBest, f_classif \n",
    "    if len(features) > 10:\n",
    "        print(\"Too many features, do SelectKBest.\")\n",
    "        selector = SelectKBest(f_classif, k = 10)\n",
    "        train_df = selector.fit_transform(train_df[features], target)\n",
    "        scores = -np.log10(selector.pvalues_)\n",
    "        indices = np.argsort(scores)[::-1]\n",
    "        new_features = []\n",
    "        for f in range(10):\n",
    "            new_features.append(features[indices[f]])\n",
    "    \n",
    "    # pack dataset for next step\n",
    "    if len(combined_train_test) == 1:\n",
    "        data = [train_df, target]\n",
    "    else:\n",
    "        data = [train_df, target, test_df]\n",
    "    \n",
    "    # display\n",
    "    print(\"Feature Selection Done!\\n\")\n",
    "    if len(features) > 10:\n",
    "        print(\"Finally, the feature selected for estimation are\", new_features, \".\\n\")\n",
    "    else:\n",
    "        print(\"Finally, the feature selected for estimation are\", features, \".\\n\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimation(row_csv, data):\n",
    "    \"\"\"\n",
    "        row_csv: a csv file containing target column index.\n",
    "        data:    a pack containing training set or/and testing set.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # display\n",
    "    print(\"Part IV: Estimation\")\n",
    "    print(\"Start estimation...\")\n",
    "    \n",
    "    # import library\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import train_test_split, cross_val_score\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "    # unpack data\n",
    "    train_df, target = data[0], data[1]\n",
    "    if len(data) == 3:\n",
    "        test_df = data[2]\n",
    "    \n",
    "    # train_test_split\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(train_df, target, test_size = 0.3)\n",
    "    \n",
    "    # get classifier\n",
    "    row = pd.read_csv(row_csv)\n",
    "    code = row[\"estimator3\"].values[0]\n",
    "    try:\n",
    "        clf = eval(code)\n",
    "    except:\n",
    "        print(\"\\nThere is something wrong with 'estimator3' in your row.csv file, please check again!\")\n",
    "        os.exit(0)\n",
    "    else:\n",
    "        print(\"\\nLoad estimator success!\")\n",
    "    \n",
    "    # cross_validation\n",
    "    score_type = row[\"performanceMetric\"].values[0]\n",
    "    scores = cross_val_score(clf, train_df, target, cv = 5, scoring = score_type)\n",
    "    \n",
    "    # display\n",
    "    print(\"Estimation Done!\\n\")\n",
    "    print(\"Classifier is \" + code + \"and the performance metric is:\", score_type)\n",
    "    print(\"\\nScore: %0.3f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), 'Classifier Cross Validation'))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocessing():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part I: Preprocessing\n",
      "Start preprocessing...\n",
      "Train Data Loading Success.\n",
      "Detected no test data!\n",
      "Row.csv Loading Success.\n",
      "Preprocessing Done!\n",
      "\n",
      "We have the target feature  ['No-show'] .\n",
      "\n",
      "The categorical features are ['Gender', 'ScheduledDay', 'AppointmentDay', 'Neighbourhood', 'Scholarship', 'Hipertension', 'Diabetes', 'Alcoholism', 'Handcap', 'SMS_received'] and numerical features are ['PatientId', 'AppointmentID', 'Age'] .\n",
      "\n",
      "Specifically in categorical features, we have features ['Gender', 'AppointmentDay', 'Neighbourhood', 'Scholarship', 'Hipertension', 'Diabetes', 'Alcoholism', 'Handcap', 'SMS_received'] , which have several unique values  and ['ScheduledDay'] , which have plenty of unique values.\n",
      "\n",
      "Part II: Feature Extraction\n",
      "Start feature extraction...\n",
      "Feature Extraction Done!\n",
      "\n",
      "We don't get new features.\n",
      "Part III: Feature Selection\n",
      "Start feature selection...\n",
      "Too many features, do SelectKBest.\n",
      "Feature Selection Done!\n",
      "\n",
      "Finally, the feature selected for estimation are ['SMS_received', 'AppointmentID', 'Age', 'Hipertension', 'Scholarship', 'AppointmentDay', 'Diabetes', 'Neighbourhood', 'Handcap', 'Gender'] .\n",
      "\n",
      "Part IV: Estimation\n",
      "Start estimation...\n",
      "\n",
      "Load estimator success!\n",
      "Estimation Done!\n",
      "\n",
      "Classifier is LogisticRegression(class_weight={0:0.8, 1:0.2})and the performance metric is: accuracy\n",
      "\n",
      "Score: 0.798 (+/- 0.00) [Classifier Cross Validation]\n",
      "--- Total runnung time 3.5894014835357666 seconds ---\n"
     ]
    }
   ],
   "source": [
    "def main(row_csv, trainData_csv, testData_csv = None):\n",
    "    \"\"\"\n",
    "        row_csv:           a csv file containing target column index.\n",
    "        trainData_csv:     training dataset, csv file.\n",
    "        testData_csv:      testing dataset, csv file.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    #\n",
    "    start_time = time.time()\n",
    "    \n",
    "    data, cols = preprocessing(row_csv, trainData_csv, testData_csv)\n",
    "    data, cols = feature_extraction(data, cols)\n",
    "    data = feature_selection(data, cols)\n",
    "    estimation(row_csv, data)\n",
    "    postprocessing()\n",
    "    \n",
    "    print(\"--- Total runnung time %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    return \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(\"submission/row.csv\", \"data/trainData.csv\", \"data/testData.csv\")\n",
    "    #main(\"Medical Appointment/submission/row.csv\", \"Medical Appointment/data/trainData.csv\", \"Medical Appointment/data/testData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
